{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Feature Selection 101\n",
    "\n",
    "Feature engineering is the process of **selecting**, **construction**, or **transforming** raw data into features that form a quality dataset that can be used in supervised and unsupervised learning.\n",
    "\n",
    "The art/science of translating data in the best way feasible involves an elegant combination of domain expertise, intuition, and mathematics. This section provides a quick overview  to the most basic yet regularly used strategies for feature engineering. \n",
    "\n",
    "\n",
    "### ML workflow\n",
    "\n",
    "A typical ML workflow/pipeline looks like this:\n",
    "\n",
    "\n",
    "![workflow](data_cycle.svg)\n",
    "\n",
    "\n",
    "There can be many ways to divide the tasks that make up the ML workflow into phases. But generally the basic steps are similar as the graph above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Exploration\n",
    "\n",
    "## What is a variable?\n",
    "\n",
    "A variable is any differentiating factor, number, or amount that can be measured or quantified. They are termed \"variables\" because the value they accept can and generally does change.\n",
    "\n",
    "### Variables types\n",
    "\n",
    "Variables come in different shapes and forms from very common and intuitive like age, sex or eye colour to complicated measures that require domain knowledge for example systolic blood pressure. \n",
    "\n",
    "It is useful to cluster variables into four major types:\n",
    "\n",
    "- Numerical: e.g. binary, integers, float or complex  \n",
    "- Categorical: e.g. nominal and ordinal  \n",
    "- Datetime: i.e. any time related unit  \n",
    "- Freetext: i.e. any observation containing unstructured textual data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Feature?\n",
    "\n",
    "- A feature is just a sequence of variables that almost always share the same type. \n",
    "- The building blocks of datasets are features.\n",
    "- The features of a dataset that you use for machine learning greatly influence the quality of the insights you will be able to derive.\n",
    "- Moreover, different scientific challenges within a given discipline may not always require the same characteristics, which is why understanding the specific goals of any data science project is paramount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Feature Characteristics?\n",
    "Any scientific endeavour needs to be able to describe features using some summary measures\n",
    "\n",
    "- The most common characteristics regardless of type is missingness. Which is just the percentage of missing data in a specific feature. \n",
    "- Then each of the four types of data will have different means of describing them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numeric Feature Characteristics\n",
    "- For example, numeric features will often be described using:\n",
    "  - Statistical moments: e.g. mean, var,  skewness and kurtosis \n",
    "  - Magnitude, range and quantiles \n",
    "  - Outliers: i.e. the existence of extreme value that is significantly different from the remaining data\n",
    "\n",
    "### Categorical Feature Characteristics\n",
    "- In contrast, categorical features will be described using:\n",
    "  - Cardinality: i.e. The number of unique labels the feature contains\n",
    "  - label frequency: i.e. the distribution of labels across a feature\n",
    "\n",
    "\n",
    "### Datetime Feature Characteristics\n",
    "- Datetime features will be described using:\n",
    "  - Min, max date\n",
    "  - Observation frequency\n",
    "  - And can also apply more complex methods such as trend, stationarity etc.\n",
    "\n",
    "\n",
    "### free text Feature Characteristics\n",
    "- Finally, free text features can be described using: \n",
    "  - Dictionary: i.e. the unique set of words that can be formed from the feature \n",
    "  - Min and Max sentence length in words \n",
    "  - If relevant we can also extract affective tendencies \n",
    "  - Or even richness of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Feature type\n",
    "\n",
    "#### Univariate Analysis\n",
    "\n",
    "Descriptive statistics on one single variable.\n",
    "\n",
    "| Variable    | Method  | \n",
    "| ----------- | ---------------- | \n",
    "| Categorical | **Shape**:<br />Histogram/ Frequency table...  |\n",
    "| Numerical   | **Central Tendency**:<br />Mean/ Median/ Mode<br />**Dispersion**:<br />Min/ Max/ Range/ Quantile/ IQR/ MAD/ Variance/ Standard Deviation/ <br />**Shape**:<br />Skewness/ Kurtosis/ Binning... |\n",
    "\n",
    "#### Pandas\n",
    "- Categorical: `<pd.Series>.value_counts()`\n",
    "- Central Tendency: `<pd.Series>.agg(['mean','median','mode'])`\n",
    "- Dispersion: `<pd.Series>.agg(['min','max','ptp','quantile','scipy.stats.iqr','mad','var','std'])`\n",
    "- Shape: `<pd.Series>.agg(['skew','kurt'])`\n",
    "\n",
    "#### Plots\n",
    "- Categorical: Bar, line, area, swarm plots\n",
    "- Numerical: Density, histogram, Box, violin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Multi-variate Analysis\n",
    "\n",
    "Descriptive statistics between two or more variables.\n",
    "Density approximation \n",
    "\n",
    "\n",
    "| Variable    | Method  | \n",
    "| ----------- | ---------------- | \n",
    "| Categorical | **Shape**:<br />ND Histogram/ Cross tabulation table...  |\n",
    "| Numerical   | **Central Tendency**:<br />... Vectors <br />**Dispersion**:<br />... Vectors <br />**Shape**:<br />... covariance/ correlation matrix |\n",
    "\n",
    "\n",
    "#### Numpy and Pandas\n",
    "- Categorical: `np.histogramdd, pd.crosstab`\n",
    "- Central Tendency: `<pd.DataFrame>.mean(axis=1)`\n",
    "- Dispersion: `<pd.DataFrame>.min(axis=1)`\n",
    "- Shape: `Morphology, distance and similarity`\n",
    "\n",
    "#### Plots\n",
    "- Categorical: Scatter, swarm Plot\n",
    "- Correlation Plot\n",
    "- Heat Maps\n",
    "- Radviz \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset Cleaning\n",
    "\n",
    "### Missing Values\n",
    "\n",
    "**Definition**: For a certain observation there is some proportion of missing variables across features.\n",
    "\n",
    "- Certain algorithms cannot work when missing value are present\n",
    "- Even for algorithm that handle missing data, without treatment the model can lead to inaccurate conclusion\n",
    "- Missingness can be evidence of a bigger problem that the experiment is neglecting to deal with\n",
    "\n",
    "\n",
    "\n",
    "### Missing Mechanisms \n",
    "\n",
    "- It is important to identify the processes that underlays the reasons for missingness in the dataset. \n",
    "- We may opt to treat the missing information differently depending on the mechanism. \n",
    "- There Are three theoretical missingness concepts\n",
    "  - Missing Completely at Random : no relationship between the data missing and any other values\n",
    "  - Missing at Random : Depends on observed Predictors\n",
    "  - Missing Not At Random : Depends on Unobserved Predictors\n",
    "\n",
    "#### Missing Completely at Random\n",
    "\n",
    "- A variable is missing completely at random (MCAR) if the probability of being missing is the same for all the observations. \n",
    "- There is **nothing systematic going on** that makes some data more likely to be missing than other.\n",
    "- If values for observations are missing completely at random, then disregarding those cases would not bias the inferences made.\n",
    "\n",
    "#### Missing at Random\n",
    "\n",
    "- Missing as Random (MAR) occurs when **there is a systematic relationship** between the propensity of missing values and the observed data. \n",
    "- In other words, the probability an observation being missing depends only on available information (other variables in the dataset), but not on the variable itself.\n",
    "- For example, if people from african ethnicity are less likely to allow their financial status to be captured on internet databases compared to european ones, ethnicity is associated with financial status. \n",
    "- And we will expect to see that the financial information will be missing at random more for africans than for europeans.\n",
    "\n",
    "\n",
    "#### Missing Not At Random \n",
    "\n",
    "- Missingness depends on information that has not been recorded, and this information also predicts the missing values.\n",
    "- If the website used to acquire data only works well for high band internet there will be a bias for urban areas where the deployment of high band internet is more abundant \n",
    "\n",
    "- In this situation, data sample is biased if we drop those missing cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for a Missing Mechanism\n",
    "\n",
    "- Using **domain expertise**. \n",
    "  - In many situations we can assume the mechanism by probing into the logic behind a feature.\n",
    "- Using **statistical test**.  \n",
    "  - Use missingness as either a binary or continues variable and test it's dependency on other features  \n",
    "  - If a dependence exists we can assume that the data is Missing at Random\n",
    "- In practice without the ability to conduct actual experiments on the missing mechanism it is very hard to be certain  \n",
    "\n",
    "\n",
    "\n",
    "### Dealing with Missing Data\n",
    "There are many ways to deal with missing data here is a list of the most common ones \n",
    "- Row wise Deletion \n",
    "  - Remove observations with any missingness\n",
    "  - This approach will preserve joint distribution if **Missing Completely at Random (MCAR)** \n",
    "  - May yield biased estimates if **Missing at Random** or **Missing Not At Random**, because we are biased towards a specific group\n",
    "  - Often data acquisition is expensive and discarding too much data is not realistic\n",
    "- Mean/Median/Mode Imputation\n",
    "  - Replacing the NaN by mean/median and the NA by most frequent value\n",
    "  - Effective for MCAR\n",
    "  - However, this will distort the distributions and the joint distributions \n",
    "- Random Imputation\n",
    "  - Replacing the NA/NaN by sampling at random from the available observations \n",
    "  - Preserve distribution if MCAR\n",
    "  - However, this introduces a hidden bias that is hard to control\n",
    "- Arbitrary Value Imputation\n",
    "  - Replacing the NA/NaN with a single number that reflects it's missingness \n",
    "  - Distort distributions \n",
    "- Add missingness feature\n",
    "  - Include a feature to indicate that the value was imputed \n",
    "  - May capture missingness dependencies\n",
    "  - Increases feature space complexity \n",
    "\n",
    "**Note**: Some methods, like as XGboost, include missing data treatment into the model-building process, eliminating the requirement for this phase. However, it is critical that you understand how the algorithm treats them and convey this in your paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "- An outlier is an observation that deviates so much from the other observations that it raises the possibility that it was created by a separate mechanism.\n",
    "- Outliers, depending on the context, either deserve special attention or should be completely ignored. \n",
    "- For example, an extreme response time on a cognitive task is usually a sign of task disengagement, while a weight of 700kg is very likely due to measurement error and should be filter out or impute with something else.\n",
    "\n",
    "\n",
    "#### What can Outlier do?\n",
    "\n",
    "The presence of outliers may:\n",
    "\n",
    "- Disrupt the functionality of most linear and distance dependent approaches \n",
    "- Introduce noises to dataset\n",
    "- Make samples less representative\n",
    "- Any algorithms that rely on means/variance are sensitive to outliers as those stats are greatly influenced by extreme values.\n",
    "\n",
    "#### Outlier Detection\n",
    "\n",
    "- Outlier analysis and anomaly detection is a huge field of research. \n",
    "- As you would expect there is a is a comprehensive Python toolkit (PyOD) to detect and deal with outliers\n",
    "- The common statistical methods to detect outliers are: \n",
    "  - Detect by arbitrary or domain boundary : just manual\n",
    "  - Mean & Standard Deviation method : $\\mu \\pm 3*sd$\n",
    "  - IQR method : $iqr=q3-q1$, $q1-iqr*1.5, q3+iqr*1.5$\n",
    "  - MAD method : $mad =  median(abs(x-q2))$, $q2 \\pm mad*2.5$\n",
    "\n",
    "However, beyond these methods, it's more important to keep in mind that the study context should govern how you define and react to these outliers. The meanings of your findings should be dictated by the underlying context, rather than the number itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Outliers\n",
    "\n",
    "- Mean/Median/Mode Imputation\n",
    "  - replacing the outlier by mean/median/most frequent values of that feature\n",
    "- Discretization\n",
    "  -  Transform the continuous feature into discrete variables\n",
    "- Windsorization\n",
    "  - Capping the maximum of a distribution at an arbitrarily set value, vice versa\n",
    "- Discard outliers\n",
    "  - Drop all the observations that are outliers\n",
    "\n",
    "### To summarise \n",
    "\n",
    "- There are many strategies for dealing with outliers in data, and depending on the context and data set, any could be the right or the wrong way. \n",
    "- It’s important to investigate the nature of the outlier before deciding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Rare Values\n",
    "\n",
    "- Categorical variable with labels with very low frequency.\n",
    "- In some situations rare values, like outliers, may contains valuable information of the dataset and therefore need particular attention. \n",
    "- For example, a hospital visit of a patient is rare but important.\n",
    "- Rare values in categorical variables tend to cause over-fitting, particularly in **tree based** methods.\n",
    "- A big number of infrequent labels adds noise, with little information, therefore causing over-fitting.\n",
    "- Rare labels may be present in training set, but not in test set, therefore causing over-fitting to the train set.\n",
    "- Rare labels may appear in the test set, and not in the train set. Thus, the model will not know how to evaluate it. \n",
    "\n",
    "#### Handling Rare Value\n",
    "\n",
    "- Mode Imputation\n",
    "  - Replacing the rare label by most frequent label\n",
    "- Grouping into one new category\n",
    "  - Grouping the observations that show rare labels into a unique category \n",
    "\n",
    "Depending on the situation, we may use different strategies:\n",
    "\n",
    "\n",
    "### High Cardinality\n",
    "\n",
    "- The number of labels within a categorical variable is known as  cardinality. \n",
    "- A high number of labels within a variable is known as high cardinality. \n",
    "- Variables with too many labels tend to dominate over those with only a few labels, particularly in **tree based** algorithms.\n",
    "- A big number of labels within a variable may introduce noise with little if any information, therefore making the machine learning models  prone to over-fit.\n",
    "- Some of the labels may only be present in the training data set,  but not in the test set, therefore causing algorithms to over-fit the training set.\n",
    "- Contrarily, new labels may appear in the test set that were not  present in the training set, therefore leaving algorithm unable to perform a calculation over the new observation.\n",
    "\n",
    "#### Handling High Cardinality\n",
    "- Grouping labels with domain knowledge  \n",
    "- Grouping labels with rare occurrence into other category\n",
    "- Grouping labels with some learning algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 3. Feature Engineering\n",
    "\n",
    "### 3.1 Feature Scaling\n",
    "\n",
    "**Definition**: Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
    "\n",
    "#### 3.1.1 Why Feature Scaling Matters\n",
    "\n",
    "- If range of inputs varies, in some algorithms, object functions will not work properly.\n",
    "\n",
    "- **Gradient descent** converges much faster with feature scaling done. Gradient descent is a common optimization algorithm used in logistic regression, SVMs,  neural networks etc.\n",
    "\n",
    "- Algorithms that involve **distance calculation** like KNN, Clustering are also affected by the magnitude of the feature. Just consider how Euclidean distance is calculated: taking the square root of the sum of the squared differences between observations. This distance can be greatly affected by differences in scale among the variables. Variables with large variances have a larger effect on this measure than variables with small variances.\n",
    "\n",
    "**Note**: Tree-based algorithms are almost the only algorithms that are not affected by the magnitude of the input, as we can easily see from how trees are built.  When deciding how to make a split, tree algorithm look for decisions like \"whether feature value X>3.0\" and compute the purity of the child node after the split, so the scale of the feature does not count.\n",
    "\n",
    "#### 3.1.2 How to Handle Feature Scaling\n",
    "\n",
    "| Method                                            | Definition                                                   | Pros                                                         | Cons                                                         |\n",
    "| ------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Normalization - Standardization (Z-score scaling) | removes the mean and scales the data to unit variance.<br />z = (X - X.mean) /  std | feature is rescaled to have a standard normal distribution that centered around 0 with SD of 1 | compress the observations in the narrow range if the variable is skewed or has outliers, thus impair the predictive power. |\n",
    "| Min-Max scaling                                   | transforms features by scaling each feature to a given range. Default to [0,1].<br />X_scaled = (X - X.min / (X.max - X.min) | /                                                            | compress the observations in the narrow range if the variable is skewed or has outliers, thus impair the predictive power. |\n",
    "| Robust scaling                                    | removes the median and scales the data according to the quantile range (defaults to IQR)<br />X_scaled = (X - X.median) / IQR | better at preserving the spread of the variable after transformation for skewed variables | /                                                            |\n",
    "\n",
    "\n",
    "\n",
    "A comparison of three methods when facing outliers:\n",
    "\n",
    "\n",
    "![scaling](/images/scaling.png)\n",
    "\n",
    "[img source](https://stackoverflow.com/questions/51841506/data-standardization-vs-normalization-vs-robust-scaler)\n",
    "\n",
    "As we can see, Normalization - Standardization and Min-Max method will compress most data to a narrow range, while robust scaler does a better job at keeping the spread of the data, although it cannot **remove** the outlier from the processed result. Remember removing/imputing outliers is another topic in data cleaning and should be done beforehand.\n",
    "\n",
    "Experience on how to choose feature scaling method:\n",
    "\n",
    "- If your feature is not Gaussian like, say, has a skewed distribution or has outliers, Normalization - Standardization is not a good choice as it will compress most data to a narrow range.\n",
    "- However, we can transform the feature into Gaussian like and then use Normalization - Standardization. Feature transformation will be discussed in section 3.4\n",
    "- When performing distance or covariance calculation (algorithm like Clustering, PCA and LDA), it is better to use Normalization - Standardization as it will remove the effect of scales on variance and covariance. Explanation [here](https://blog.csdn.net/zbc1090549839/article/details/44103801).\n",
    "- Min-Max scaling has the same drawbacks as Normalization - Standardization, and also new data may not be bounded to [0,1] as they can be out of the original range. Some algorithms, for example some deep learning network prefer input on a 0-1 scale so this is a good choice.\n",
    "\n",
    "\n",
    "\n",
    "Below is some additional resource on this topic:\n",
    "\n",
    "- A comparison of the three methods when facing skewed variables can be found [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py).\n",
    "- An in-depth study of feature scaling can be found [here](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html).\n",
    "\n",
    "\n",
    "\n",
    "### 3.2 Discretize\n",
    "\n",
    "**Definition**: Discretization is the process of transforming continuous variables into discrete variables by creating a set of contiguous intervals that spans the range of the variable's values.\n",
    "\n",
    "#### 3.2.1 Why Discretize Matters\n",
    "\n",
    "- help to improve model performance by grouping of similar attributes with similar predictive strengths\n",
    "- bring into non-linearity and thus improve fitting power of the model\n",
    "- enhance interpretability with grouped values\n",
    "- minimize the impact of **extreme values/seldom reversal patterns**\n",
    "- prevent overfitting possible with numerical variables\n",
    "- allow feature interaction between continuous variables (section 3.5.5)\n",
    "\n",
    "\n",
    "\n",
    "#### 3.2.2 How to Handle Discretization\n",
    "\n",
    "| Method                              | Definition                                                   | Pros                                                         | Cons                                                         |\n",
    "| ----------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Equal width binning                 | divides the scope of possible values into N bins of the same width | /                                                            | sensitive to skewed distribution                             |\n",
    "| Equal frequency binning             | divides the scope of possible values of the variable into N bins, where each bin carries the same amount of observations | may help boost the algorithm's performance                   | this arbitrary binning may disrupt the relationship with the target |\n",
    "| K-means binning                     | using k-means to partition values into clusters              | /                                                            | needs hyper-parameter tuning                                 |\n",
    "| Discretization using decision trees | using a decision tree to identify the optimal splitting points that would determine the bins | observations within each  bin are more similar to themselves than to those of other bins | 1. may cause over-fitting<br>2. may not get a good performing tree |\n",
    "| ChiMerge[11]                       | supervised hierarchical bottom-up (merge) method that locally exploits the chi-square criterion to decide whether two adjacent intervals are similar enough to be merged | robust and make use of a priori knowledge                    | cannot handle unlabeled data                                 |\n",
    "\n",
    "In general there's no best choice of discretization method. It really depends on the dataset and the following learning algorithm. Study carefully about your features and context before deciding. You can also try different methods and compare the model performance.\n",
    "\n",
    "Some literature reviews on feature discretization can be found [here1](https://pdfs.semanticscholar.org/94c3/d92eccbb66f571153f99b7ae6c6167a00923.pdf), [here2](http://robotics.stanford.edu/users/sahami/papers-dir/disc.pdf), [here3](http://axon.cs.byu.edu/papers/ventura.thesis.ps).\n",
    "\n",
    "\n",
    "\n",
    "### 3.3 Feature Encoding\n",
    "\n",
    "#### 3.3.1 Why Feature Encoding Matters\n",
    "\n",
    "We must transform strings of categorical variables into numbers so that algorithms can handle those values. Even if you see an algorithm can take into categorical inputs, it's most likely that the algorithm incorporate the encoding process within.\n",
    "\n",
    "#### 3.3.2 How to Handle Feature Encoding\n",
    "\n",
    "| Method                   | Definition                                                   | Pros                                                         | Cons                                                         |\n",
    "| ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| One-hot encoding         | replace the categorical variable by different boolean variables (0/1) to indicate whether or not certain label is true for that observation | keep all information of that variable                        | 1. expand feature space dramatically if too many labels in that variable<br />2. does not add additional value to make the variable more predictive |\n",
    "| Ordinal-encoding         | replace the labels by some ordinal number if ordinal is meaningful | straightforward                                              | does not add additional value to make the variable more predictive |\n",
    "| Count/frequency encoding | replace each label of the categorical variable by the count/frequency within that category | /                                                            | 1. may yield same encoding for two different labels (if they appear same times) and lose valuable info.<br />2. may not add predictive power |\n",
    "| Mean encoding     | replace the label by the mean of the target for that label. (the target must be 0/1 valued or continuous) | 1. Capture information within the label, therefore rendering more predictive features<br/>2. Create a monotonic relationship between the variable and the target<br>3. Do not expand the feature space | Prone to cause over-fitting                                  |\n",
    "| WOE encoding[9]         | replace the label  with Weight of Evidence of each label. WOE is computed from the basic odds ratio: ln( (Proportion of Good Outcomes) / (Proportion of Bad Outcomes)) | 1. Establishes a monotonic relationship to the dependent variable<br/>2. Orders the categories on a \"logistic\" scale which is natural for logistic regression<br>3，The transformed variables, can then be compared because they are on the same scale. Therefore, it is possible to determine which one is more predictive. | 1. May incur in loss of information (variation) due to binning to few categories<br/>2. Prone to cause over-fitting |\n",
    "| Target encoding[10]     | Similar to mean encoding, but use both posterior probability and prior probability of the target | 1. Capture information within the label, therefore rendering more predictive features<br/>2. Create a monotonic relationship between the variable and the target<br/>3. Do not expand the feature space | Prone to cause over-fitting      |\n",
    "\n",
    "**Note**: if we are using one-hot encoding in linear regression, we should keep k-1 binary variable to avoid multicollinearity. This is true for any algorithms that look at all features at the same time during training. Including SVM, neural network and clustering. Tree-based algorithm, on the other hand, need the entire set of binary variable to select the best split.\n",
    "\n",
    "**Note**: it is not recommended to use one-hot encoding with tree algorithms. One-hot will cause the split be highly imbalanced (as each label of the original categorical feature will now be a new feature), and the result is that neither of the two child nodes will have a good gain in purity. The prediction power of the one-hot feature will be weaker than the original feature as they have been broken into many pieces.\n",
    "\n",
    "An in-detail intro to WOE can be found [here](http://documentation.statsoft.com/StatisticaHelp.aspx?path=WeightofEvidence/WeightofEvidenceWoEIntroductoryOverview).\n",
    "\n",
    "\n",
    "\n",
    "### 3.4 Feature Transformation\n",
    "\n",
    "#### 3.4.1 Why Feature Transformation Matters\n",
    "\n",
    "##### 3.4.1.1 Linear Assumption\n",
    "\n",
    "**Regression**\n",
    "\n",
    "Linear regression is a straightforward approach for predicting a  quantitative response Y on the basis of a different predictor variable  X1, X2, ... Xn. It assumes that there is a linear relationship between  X(s) and Y. Mathematically, we can write this linear relationship as Y ≈  β0 + β1X1 + β2X2 + ... + βnXn. \n",
    "\n",
    "**Classification**\n",
    "\n",
    "Similarly, for classification, Logistic Regression assumes a linear relationship between the variables and the log of the odds.\n",
    "\n",
    "Odds = p / (1 - p), where p is the probability of y = 1\n",
    "\n",
    "log(odds) = β0 + β1X1 + β2X2 + ... + βnXn\n",
    "\n",
    "**Why it's important to follow linear assumption**\n",
    "\n",
    "If the machine learning model assumes a linear dependency between the predictors Xs and the outcome Y, when there is not such a linear relationship, the model will have a poor performance. In such cases, we are better off trying another machine learning model that does not make such assumption.\n",
    "\n",
    "If there is no linear relationship and we have to use the linear/logistic regression models, mathematical transformation/discretization may help create the relationship, though it cannot guarantee a better result.\n",
    "\n",
    "##### 3.4.1.2 Variable Distribution\n",
    "\n",
    "**Linear Regression Assumptions**\n",
    "\n",
    "Linear Regression has the following assumptions over the predictor variables X:\n",
    "\n",
    "- Linear relationship with the outcome Y\n",
    "\n",
    "- Multivariate normality\n",
    "- No or little multicollinearity\n",
    "- Homoscedasticity\n",
    "\n",
    "Normality assumption means that every variable X should follow a Gaussian distribution.\n",
    "\n",
    "Homoscedasticity, also known as homogeneity of variance, describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables (Xs) and the dependent variable (Y)) is the same across all values of the independent variables.\n",
    "\n",
    "Violations in the assumptions of homoscedasticity and / or normality (assuming a distribution of data is homoscedastic or Gaussian, when in reality it is not) may result in poor model performance.\n",
    "\n",
    "The remaining machine learning models, including Neural Networks, Support Vector Machines, Tree based methods and PCA do not make any assumption over the distribution of the independent variables. However, in many occasions the model performance may **benefit from a \"Gaussian-like\" distribution**.\n",
    "\n",
    "Why may models benefit from a \"Gaussian-like\" distributions? In variables with a normal distribution, the observations of X available to predict Y vary across a greater range of values, that is, the values of X are \"spread\" over a greater range.\n",
    "\n",
    "In the situations above, transformation of the original variable can help give the variable more of a bell-shape of the Gaussian distribution.\n",
    "\n",
    "#### 3.4.2 How to Handle Feature Transformation\n",
    "\n",
    "| Method                      | Definition                                               |\n",
    "| --------------------------- | -------------------------------------------------------- |\n",
    "| Logarithmic transformation  | log(x+1).  We use (x+1) instead of x to avoid value of 0 |\n",
    "| Reciprocal transformation   | 1/x. Warning that x should not be 0.                     |\n",
    "| Square root transformation  | x**(1/2)                                                 |\n",
    "| Exponential transformation  | X**(m)                                                   |\n",
    "| Box-cox transformation[12] | (X**λ-1)/λ                                               |\n",
    "| Quantile transformation     | transform features using quantiles information           |\n",
    "\n",
    "**Log transformation** is useful when applied to skewed distributions as they tend to expand the values which fall in the range of lower magnitudes and tend to compress or reduce the values which fall in the range of higher magnitudes, which helps to make the skewed distribution as normal-like as possible. **Square root transformation** does a similar thing in this sense.\n",
    "\n",
    "**Box-Cox transformation** in sklearn [13] is another popular function belonging to the power transform family of functions. This function has a pre-requisite that the numeric values to be transformed must be positive (similar to what log transform expects). In case they are negative, shifting using a constant value helps. Mathematically, the Box-Cox transform function can be denoted as follows.\n",
    "\n",
    "![](images/box-cox.png)\n",
    "\n",
    "**Quantile transformation** in sklearn [14] transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme. However, this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.\n",
    "\n",
    "\n",
    "\n",
    "We can use **Q-Q plot** to check if the variable is normally distributed (a 45 degree straight line of the values over the theoretical quantiles) after transformation.\n",
    "\n",
    "Below is an example showing the effect of sklearn's Box-plot/Yeo-johnson/Quantile transform to map data from various distributions to a normal distribution.\n",
    "\n",
    "![](images/sphx_glr_plot_map_data_to_normal_001.png)\n",
    "\n",
    "[img source](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_map_data_to_normal.html#sphx-glr-auto-examples-preprocessing-plot-map-data-to-normal-py) \n",
    "\n",
    "On “small” datasets (less than a few hundred points), the quantile transformer is prone to overfitting. The use of the power transform is then recommended.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3.5 Feature Generation\n",
    "\n",
    "**Definition**: Creating new features as a combination of existing ones. It's a great way to add domain knowledge to the dataset.\n",
    "\n",
    "#### 3.5.1 Missing Data Derived Feature\n",
    "\n",
    "As mentioned in section 2.1, we can create new binary feature denoting whether the observations have missing value on raw feature with value 0/1.\n",
    "\n",
    "#### 3.5.2 Simple Statistical Derived Feature\n",
    "\n",
    "Creating new features by performing simple statistical calculations on the raw features, including:\n",
    "\n",
    " - count/sum\n",
    " - average/median/mode\n",
    " - max/min/stddev/variance/range/IQR/Coefficient of Variation\n",
    " - time span/interval\n",
    "\n",
    "Take call log for example, we can create new features like: number of calls, number of call-in/call-out, average calling duration, monthly average calling duration, max calling duration, etc.\n",
    "\n",
    "#### 3.5.3 Feature Crossing\n",
    "\n",
    "After having some simple statistical derived features, we can have them crossed together. Common dimensions used for crossing include:\n",
    "\n",
    "- time\n",
    "- region\n",
    "- business types\n",
    "\n",
    "Still take call log for example, we can have crossed features like: number of calls during night times/day times, number of calls under different business types (banks/taxi services/travelling/hospitalities), number of calls during the past 3 months, etc. Many of the statistical calculations mentioned in section 3.5.2 can be used again to create more features.\n",
    "\n",
    "**Note**: An open-source python framework named **Featuretools** that helps automatically generate such features can be found [here](https://github.com/Featuretools/featuretools). \n",
    "\n",
    "![featuretools](images/featuretools.png)\n",
    "\n",
    "Personally I haven't used it in practice. You may try and discover if it can be of industry usage.\n",
    "\n",
    "#### 3.5.4 Ratios and Proportions\n",
    "\n",
    "Common techniques. For example, in order to predict future performance of credit card sales of a branch, ratios like credit card sales / sales person or credit card sales / marketing spend would be more powerful than just using absolute number of card sold in the branch.\n",
    "\n",
    "#### 3.5.5 Cross Products between Categorical Features\n",
    "\n",
    "Consider a categorical feature A, with two possible values {A1, A2}. Let B be a feature with possibilities {B1, B2}. Then, a feature-cross between A & B  would take one of the following values: {(A1, B1), (A1, B2), (A2, B1), (A2, B2)}. You can basically give these ‘combinations’ any names you like. Just remember that every combination denotes a synergy between the information contained by the corresponding values of A and B.\n",
    "\n",
    "This is an extremely useful technique, when certain features together denote a property better than individually by themselves. Mathematically speaking, you are doing a cross product between all possible values of the categorical features. The concepts is similar to Feature Crossing of section 3.5.3, but this one particularly refers to the crossing between 2 categorical features.\n",
    "\n",
    "#### 3.5.6 Polynomial Expansion\n",
    "\n",
    "The cross product can also be applied to numerical features, which results in a new interaction feature between A and B. This can be done easily be sklearn's  [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures), which generate a new feature set consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, three raw features {X1, X2, X3} can generate a feature set of  {1, X1X2, X1X3, X2X3, X1X2X3} with a degree of 2.\n",
    "\n",
    "#### 3.5.7 Feature Learning by Trees\n",
    "\n",
    "In tree-based algorithms, each sample will be assigned to a particular leaf node. The decision path to each node can be seen as a new non-linear feature, and we can create N new binary features where n equals to the total number of leaf nodes in a tree or tree ensembles. The features can then be fed into other algorithms such as logistic regression.\n",
    "\n",
    "The idea of using tree algorithm to generate new features is first introduced by Facebook in this [paper](http://quinonero.net/Publications/predicting-clicks-facebook.pdf).\n",
    "\n",
    "The good things about this method is that we can get a complex combinations of several features together, which is informative (as is constructed by the tree's learning algorithm). This saves us much time compared to doing feature crossing manually, and is widely used in CTR (click-through rate) of online advertising industry.\n",
    "\n",
    "#### 3.5.8 Feature Learning by Deep Networks\n",
    "\n",
    "As we can see from all above, feature generation by manual takes lots of effort and may not guarantee good returns, particular when we have huge amounts of features to work with. Feature learning with trees can be seen as an early attempt in creating features automatically, and with the deep learning methods come into fashion from around 2016, they also have achieved some success in this area, such as **autoencoders** and **restricted Boltzmann machines**. They have been shown to automatically and in a unsupervised or semi-supervised way, learn abstract representations of features (a compressed form), that in turn have supported state-of-the-art results in domains such as speech recognition, image classification, object recognition and other areas. However, such features have limited interpretability and deep learning require much more data to be able to extract high quality result.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Feature Selection\n",
    "\n",
    "**Definition**:  Feature Selection is the process of selecting a subset of relevant features for use in machine learning model building. \n",
    "\n",
    "It is not always the truth that the more data, the better the result will be. Including irrelevant features (the ones that are just unhelpful to the prediction) and redundant features (irrelevant in the presence of others) will only make the learning process overwhelmed and easy to cause overfitting.\n",
    "\n",
    "With feature selection, we can have:\n",
    "\n",
    "- simplification of models to make them easier to interpret\n",
    "- shorter training times and lesser computational cost\n",
    "- lesser cost in data collection\n",
    "- avoid the curse of dimensionality\n",
    "- enhanced generalization by reducing overfitting \n",
    "\n",
    "We should keep in mind that different feature subsets render optimal performance for different algorithms. So it's not a separate process along with the machine learning model training. Therefore, if we are selecting features for a linear model, it is better to use selection procedures targeted to those models, like importance by regression coefficient or Lasso. And if we are selecting features for trees, it is better to use tree derived importance.\n",
    "\n",
    "\n",
    "\n",
    "### 4.1 Filter Method\n",
    "\n",
    "Filter methods select features based on a performance measure regardless of the ML algorithm later employed.\n",
    "\n",
    "Univariate filters evaluate and rank a single feature according to a certain criteria, while multivariate filters evaluate the entire feature space. Filter methods are:\n",
    "\n",
    "- selecting variable regardless of the model\n",
    "- less computationally expensive\n",
    "- usually give lower prediction performance\n",
    "\n",
    "As a result, filter methods are suited for a first step quick screen and removal of irrelevant features.\n",
    "\n",
    "| Method                    | Definition                                                   |\n",
    "| ------------------------- | ------------------------------------------------------------ |\n",
    "| Variance                  | removing features that show the same value for the majority/all of the observations (constant/quasi-constant features) |\n",
    "| Correlation               | remove features that are highly correlated with each other   |\n",
    "| Chi-Square                | Compute chi-squared stats between each non-negative feature and class |\n",
    "| Mutual Information Filter | Mutual information measures how much information the presence/absence of a feature contributes to making the correct prediction on Y. |\n",
    "| Univariate ROC-AUC or MSE | builds one decision tree per feature, to predict the target, then make predictions and ranks the features according to the machine learning metric (roc-auc or mse) |\n",
    "| Information Value (IV)    | a byproduct of WOE. <br>IV = Σ(Proportion of Good Outcomes - Proportion of Bad Outcomes) * WOE |\n",
    "\n",
    "WOE encoding (see section 3.3.2) and IV often go hand in hand in scorecard development. The two concepts both derived from logistic regression and is kind of standard practice in credit card industry.  IV is a popular and widely used measure as there are very convenient rules of thumb for variables selection associated with IV as below:\n",
    "\n",
    "![](images/IV.png)\n",
    "\n",
    "However, all these filtering methods fail to consider the interaction between features and may reduce our predict power. Personally I only use variance and correlation to filter some absolutely unnecessary features.\n",
    "\n",
    "\n",
    "\n",
    "**Note**: One thing to keep in mind when using chi-square test or univariate selection methods, is that in very big datasets, most of the features will show a small p_value, and therefore look like they are highly predictive. This is in fact an effect of the sample size. So care should be taken when selecting features using these procedures. An ultra tiny p_value does not highlight an ultra-important feature, it rather indicates that the dataset contains too many samples. \n",
    "\n",
    "**Note**: Correlated features do not necessarily affect model performance (trees, etc), but high dimensionality does and too many features hurt model interpretability. So it's always better to reduce correlated features.\n",
    "\n",
    "\n",
    "\n",
    "### 4.2 Wrapper Method\n",
    "\n",
    "Wrappers use a search strategy to search through the space of possible feature subsets and evaluate each subset by the quality of the performance on a ML algorithm. Practically any combination of search strategy and algorithm can be used as a wrapper. It is featured as:\n",
    "\n",
    "- use ML models to score the feature subset\n",
    "- train a new model on each subset\n",
    "- very computationally expensive\n",
    "- usually provide the best performing subset for a give ML algorithm, but probably not for another\n",
    "- need an arbitrary defined stopping criteria\n",
    "\n",
    "The most common **search strategy** group is Sequential search, including Forward Selection, Backward Elimination and Exhaustive Search. Randomized search is another popular choice, including Evolutionary computation algorithms such as genetic, and Simulated annealing.\n",
    "\n",
    "Another key element in wrappers is **stopping criteria**. When to stop the search? In general there're three:\n",
    "\n",
    "- performance increase\n",
    "- performance decrease\n",
    "- predefined number of features is reached\n",
    "\n",
    "\n",
    "\n",
    "#### 4.2.1 Forward Selection\n",
    "\n",
    "Step forward feature selection starts by evaluating all features individually and selects the one that generates the best performing algorithm, according to a pre-set evaluation criteria. In the second step, it evaluates all possible combinations of the selected feature and a second feature, and selects the pair that produce the best performing algorithm based on the same pre-set criteria.\n",
    "\n",
    "The pre-set criteria can be the roc_auc for classification and the r squared for regression for example. \n",
    "\n",
    "This selection procedure is called greedy, because it evaluates all possible single, double, triple and so on feature combinations. Therefore, it is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n",
    "\n",
    "There is a special package for python that implements this type of feature selection: [mlxtend](https://github.com/rasbt/mlxtend).\n",
    "\n",
    "#### 4.2.2 Backward Elimination\n",
    "\n",
    "Step backward feature selection starts by fitting a model using all features. Then it removes one feature. It will remove the one that produces the highest performing algorithm (least statistically significant) for a certain evaluation criteria. In the second step, it will remove a second feature, the one that again produces the best performing algorithm. And it proceeds, removing feature after feature, until a certain criteria is met.\n",
    "\n",
    "The pre-set criteria can be the roc_auc for classification and the r squared for regression for example. \n",
    "\n",
    "#### 4.2.3 Exhaustive Feature Selection\n",
    "\n",
    "In an exhaustive feature selection the best subset of features is selected, over all possible feature subsets, by optimizing a specified performance metric for a certain machine learning algorithm. For example, if the classifier is a logistic regression and the dataset consists of **4** features, the algorithm will evaluate all **15** feature combinations as follows:\n",
    "\n",
    "- all possible combinations of 1 feature\n",
    "- all possible combinations of 2 features\n",
    "- all possible combinations of 3 features\n",
    "- all the 4 features\n",
    "\n",
    "and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.\n",
    "\n",
    "This exhaustive search is very computationally expensive. In practice for this computational cost, it is rarely used.\n",
    "\n",
    "#### 4.2.4 Genetic Algorithm\n",
    "\n",
    "TODO\n",
    "\n",
    "\n",
    "\n",
    "### 4.3 Embedded Method\n",
    "\n",
    "Embedded Method combine the advantages of the filter and wrapper methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification at same time. Common embedded methods include Lasso and various types of tree-based algorithms. It is featured as:\n",
    "\n",
    "- perform feature selection as part of the model building process\n",
    "- consider interactions between features\n",
    "- less computationally expensive as it only train the model once, compared to Wrappers\n",
    "- usually provide the best performing subset for a give ML algorithm, but probably not for another\n",
    "\n",
    "\n",
    "\n",
    "#### 4.3.1 Regularization with Lasso\n",
    "\n",
    "Regularization consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model. Hence, the model will be less likely to fit the noise of the training data so less likely to be overfitting.\n",
    "\n",
    "In linear model regularization, the penalty is applied over the coefficients that multiply each of the predictors. For linear models there are in general 3 types of regularization:\n",
    "\n",
    "- L1 regularization (Lasso)\n",
    "- L2 regularization (Ridge)\n",
    "- L1/L2 (Elastic net)\n",
    "\n",
    "From the different types of regularization, **Lasso (L1)** has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model.\n",
    "\n",
    "Both for linear and logistic regression we can use the Lasso regularization to remove non-important features. Keep in mind that increasing the penalization will increase the number of features removed. Therefore, you will need to keep an eye and monitor that you don't set a penalty too high so that to remove even important features, or too low and then not remove non-important features.\n",
    "\n",
    "Having said this, if the penalty is too high and important features are removed, you should notice a drop in the performance of the algorithm and then realize that you need to decrease the regularization.\n",
    "\n",
    "Regularization is a large topic. For for information you can refer to here:\n",
    "\n",
    "- [Least angle and l1 penalised regression: A review](https://projecteuclid.org/download/pdfview_1/euclid.ssu/1211317636)\n",
    "- [Penalised feature selection and classification in bioinformatics](https://www.ncbi.nlm.nih.gov/pubmed/18562478)\n",
    "- [Feature selection for classification: A review](https://web.archive.org/web/20160314145552/http://www.public.asu.edu/~jtang20/publication/feature_selection_for_classification.pdf)\n",
    "\n",
    "- [Machine Learning Explained: Regularization](https://www.r-bloggers.com/machine-learning-explained-regularization/)\n",
    "\n",
    "\n",
    "\n",
    "#### 4.3.2 Random Forest Importance\n",
    "\n",
    "Random forests are one of the most popular machine learning algorithms.  They are so successful because they provide in general a good predictive  performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n",
    "\n",
    "Random forest is a bagging algorithm consists a bunch of base estimators (decision trees), each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are **de-correlated** and therefore **less prone to over-fitting.** \n",
    "\n",
    "Each tree is also a sequence of yes-no questions based on a single or combination of features. At each split, the question divides the dataset into 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived by how  \"**pure**\" each of the buckets is. \n",
    "\n",
    "For classification, the measure of impurity is either the **Gini impurity** or the **information gain/entropy**. For regression the measure of impurity is **variance**. Therefore, when training a tree, it is possible to compute how much each feature decreases the impurity. The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease from each feature can be averaged across trees to determine the final importance of the variable.\n",
    "\n",
    "Selecting features by using tree derived feature importance is a very  straightforward, fast and generally accurate way of selecting good features for machine learning. In particular, if you are going to build tree methods.\n",
    "\n",
    "However, correlated features will show in a tree similar and lowered importance, compared to what their importance would be if the tree was built without correlated counterparts.\n",
    "\n",
    "**Limitation**\n",
    "\n",
    "- correlated features show similar importance\n",
    "\n",
    "- correlated features importance is lower than real importance, when tree is build without its correlated counterparts\n",
    "\n",
    "- high carinal variable tend to show higher importance\n",
    "\n",
    "\n",
    "#### 4.3.3 Gradient Boosted Trees Importance\n",
    "\n",
    "Similarly to selecting features using Random Forests derived feature importance, we can select features based on the importance derived by gradient boosted trees. And we can do that in one go, or in a recursive manner, depending on how much time we have, how many features are in the dataset, and whether they are correlated or not.\n",
    "\n",
    "\n",
    "\n",
    "### 4.4 Feature Shuffling\n",
    "\n",
    "A popular method of feature selection consists in random shuffling the values of a specific variable and determining how that permutation affects the performance metric of the machine learning algorithm. In other words, the idea is to permute the values of each feature, one at the time, and measure how much the permutation decreases the accuracy, or the roc_auc, or the mse of the machine learning model. If the variables are important, this is, highly predictive, a random permutation of their values will decrease dramatically any of these metrics. Contrarily, non-important / non-predictive variables, should have little to no effect on the model performance metric we are assessing.\n",
    "\n",
    "\n",
    "\n",
    "### 4.5 Hybrid Method\n",
    "\n",
    "#### 4.5.1 Recursive Feature Elimination\n",
    "\n",
    "This method consists of the following steps:\n",
    "\n",
    "1. Rank the features according to their importance derived from a machine learning algorithm: it can be tree importance, or LASSO / Ridge,  or the linear / logistic regression coefficients.\n",
    "2. Remove one feature -the least important- and build a machine learning algorithm utilizing the remaining features.\n",
    "\n",
    "3. Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "4. If the metric decreases by more of an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature.\n",
    "5. Repeat steps 2-4 until all features have been removed (and therefore evaluated) and the drop in performance assessed.\n",
    "\n",
    "The method combines the selection process like wrappers and feature importance derivation from ML models like embedded methods so it's called hybrid.\n",
    "\n",
    "The difference between this method and the step backwards feature  selection lies in that it does not remove all features first in order to determine which one to remove. It removes the least important one, based on the machine learning model derived importance. And then, it makes an assessment as to whether that feature should be removed or not. So it removes each feature only once during selection, whereas step backward feature selection removes all the features at each step of selection.\n",
    "\n",
    "This method is therefore faster than wrapper methods and generally better than embedded methods. In practice it works extremely well. It does also account for correlations (depending on how stringent you set the arbitrary performance drop threshold). On the downside, the drop in performance assessed to decide whether the feature should be kept or removed, is set arbitrarily. The smaller the drop the more features will be selected, and vice versa.\n",
    "\n",
    "\n",
    "\n",
    "**Example: Recursive Feature Elimination with Random Forests Importance**\n",
    "\n",
    "As we talked about in section 4.3.2, Random Forests assign equal or similar importance to features that are highly correlated. In addition, when features are correlated, the  importance assigned is lower than the importance attributed to the feature itself, should the tree be built without the correlated counterparts.\n",
    "\n",
    "Therefore, instead of eliminating features based on importance **at one time** (from all initial features), we may get a better selection by removing one feature **recursively**, and recalculating the importance on each round.\n",
    "\n",
    "In this situation, when a feature that is highly correlated to another one is removed, then, the importance of the remaining feature increases. This may lead to a better subset feature space selection. On the downside, building several random forests is quite time consuming, in particular if the dataset contains a high number of features.\n",
    "\n",
    "#### 4.5.2 Recursive Feature Addition\n",
    "\n",
    "This method consists of the following steps:\n",
    "\n",
    "1. Rank the features according to their importance derived from a  machine learning algorithm: it can be tree importance, or LASSO / Ridge,  or the linear / logistic regression coefficients.\n",
    "2. Build a machine learning model with only 1 feature, the most important one, and calculate the model metric for performance.\n",
    "\n",
    "3. Add one feature -the most important- and build a machine learning  algorithm utilizing the added and any feature from previous rounds.\n",
    "\n",
    "4. Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "\n",
    "5. If the metric increases by more than an arbitrarily set threshold,  then that feature is important and should be kept. Otherwise, we can  remove that feature.\n",
    "\n",
    "6. Repeat steps 2-5 until all features have been removed (and therefore evaluated) and the drop in performance assessed.\n",
    "\n",
    "The difference between this method and the step forward feature selection  is similar. It does not look for all features first in order to determine which one to add, so it's faster than wrappers.\n",
    "\n",
    "\n",
    "\n",
    "### 4.6 Dimensionality Reduction and feature extraction\n",
    "\n",
    "- PCA\n",
    "\n",
    "- ICA\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Data Leakage\n",
    "\n",
    "This section is a remainder to myself as I have had made huge mistakes because of not aware of the problem. Data leakage is when information from outside the training dataset is used to create the model [15]. The result is that you may be creating overly optimistic models that are practically useless and cannot be used in production. The model shows great result on both your training and testing data but in fact it's not because your model really has a good generalizability but it uses information from the test data.\n",
    "\n",
    "While it is well known to use cross-validation or at least separate a validation set in training and evaluating the models, people may easily forget to do the same during the feature engineering & selection process. Keep in mind that the test dataset must not be used in any way to make choices about the model, including feature engineering & selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to expand you knowledge \n",
    "\n",
    "- [The impact of missing data on different ML algorithm](http://core.ecu.edu/omgt/krosj/IMDSDataMining2003.pdf).\n",
    "- [Inference and Missing Data](https://www.jstor.org/stable/2335739?seq=1)\n",
    "- [Identification of Outliers](https://link.springer.com/book/10.1007/978-94-015-3994-4?noAccess=true)\n",
    "- [Outlier Analysis](https://link.springer.com/book/10.1007/978-1-4614-6396-2)\n",
    "- [Python toolkit for detecting outlying objects](https://pyod.readthedocs.io/en/latest/)\n",
    "- [Outlier Detection Methods](https://docs.oracle.com/cd/E40248_01/epm.1112/cb_statistical/frameset.htm?ch07s02s10s01.html)\n",
    "- [High-Cardinality Categorical Attributes in Classification and Prediction Problems.](https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf)\n",
    "- [ChiMerge: discretization of numeric attributes](https://dl.acm.org/doi/10.5555/1867135.1867154)\n",
    "- [Feature-engine:A Python library for Feature Engineering and Selection](https://feature-engine.readthedocs.io/en/latest/)\n",
    "- [sklearn.preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Data Leakage in Machine Learning](https://machinelearningmastery.com/data-leakage-machine-learning/)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af4ed3fa87ab5389ed0d164f2907ab733f2066e3ac536df37d19ad0ca2200646"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('SYS_2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
