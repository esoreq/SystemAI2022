{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering 101 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is a variable?\n",
    "\n",
    "A variable is any differentiating factor, number, or amount that can be measured or quantified. They are termed \"variables\" because the value they accept can and generally does change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Variables types\n",
    "\n",
    "Variables come in different shapes in form from very common and intuitive like age, sex or eye colour to complicated measures that require domain knowledge for example systolic blood pressure. \n",
    "It is useful to cluster variables into four major types:\n",
    "    - Numerical: e.g. binary, integers, float or complex \n",
    "    - Categorical: e.g. nominal and ordinal \n",
    "    - Datetime: i.e. any time related unit \n",
    "    - Freetext: i.e. any observation containing unstructured textual data\n",
    "\n",
    "```{note}\n",
    "In reality there is a fifth type that is called mixed type when you combine categorical and numbers - however, this is a very domain specific use case and can be treated as Categorical with either order or no order defined.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is a Feature?\n",
    "\n",
    "A feature is just a sequence of variables that almost always share the same type. \n",
    "The building blocks of datasets are features.\n",
    "The features of a dataset that you use for machine learning greatly influence the quality of the insights you will be able to derive.\n",
    "Moreover, different scientific challenges within a given discipline may not always require the same characteristics, which is why understanding the specific goals of any data science project is paramount.\n",
    "\n",
    "The feature selection and feature engineering process, which are notoriously difficult and time-consuming, might help you improve the quality of your dataset's features. If these strategies are correctly applied, the ideal dataset will contain all the crucial attributes relevant to your unique business challenge, resulting in the best possible model outputs and the most valuable insights.\n",
    "\n",
    "Working with features is one of the most time-consuming aspects of conventional data science.\n",
    "Any project begins with identifying the data type of each feature (i.e. numerical, categorical, DateTime, or free text).\n",
    "To identify potential problems, perform fundamental statistical analysis on each feature (mean, median, standard deviation, etc.).\n",
    "It is also often valuable for producing exploratory visualisations such as histograms, frequent values charts, and count of occurrence tables for each feature, enabling you to comprehend your data rapidly and what insights it may yield.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What are Feature Characteristics?\n",
    "Any scientific endeavour needs to be able to describe features using some summary measures\n",
    "\n",
    "- The most common characteristics regardless of type is missingness. Which is just the percentage of missing data in a specific feature. \n",
    "- Then each of the four types of data will have different means of describing them. \n",
    "- For example, numeric features will often be described using:\n",
    "  - Statistical moments: e.g. mean, var,  skewness and kurtosis \n",
    "  - Magnitude, range and quantiles \n",
    "  - Outliers: i.e. the existence of extreme value that is significantly different from the remaining data\n",
    "- In contrast, categorical features will be described using:\n",
    "  - Cardinality: i.e. The number of unique labels the feature contains\n",
    "  - label frequency: i.e. the distribution of labels across a feature\n",
    "- Datetime features will be described using:\n",
    "  - Min, max date\n",
    "  - Observation frequency\n",
    "  - And can also apply more complex methods such as trend, stationarity etc.\n",
    "- Finally, free text features can be described using: \n",
    "  - Dictionary: i.e. the unique set of words that can be formed from the feature \n",
    "  - Min and Max sentence length in words \n",
    "  - If relevant we can also extract affective tendencies \n",
    "  - Or even richness of vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are quality datasets\n",
    "\n",
    "Quality datasets have the following properties:\n",
    "\n",
    "1. **Data-Type Constraints**: all column values assigned the correct datatype, i.e., binary, categorical, ordinal, numeric, date\n",
    "1. **Mandatory Columns**: some columns cannot be empty.\n",
    "1. **Range Constraints**: numeric or dates columns should fall within a certain domain range.\n",
    "1. **Categorical Consistency**: Categorical columns have a set of unique values For example, a personâ€™s sex may be male or female.\n",
    "1. **Cross-Column Consistency**: There are some interactions that need to make sense (e.g. age in years should be assoicated to birth date)\n",
    "1. **Uniformity Constraints**: The degree to which the data is specified using the same unit of measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data Preprocessing ?\n",
    "The process of data preprocessing involves converting raw data into a format that can be analysed and used. Data from the real world tends to be incomplete, inconsistent, and/or lacking in certain behaviors and trends, as well as containing a lot of errors. This can be resolved through data cleaning.\n",
    "Furthermore, when you have access to a large dataset with several potential interactions, you may wish to isolate a question of interest, doing so from the beginning will help you focus on what is most important at that time with the flexibility to always go back and incorporate the features or domains you excluded earlier.\n",
    "\n",
    "## Why use Data Preprocessing?\n",
    "In the real world, data often contain inaccuracies, noise, and inconsistencies. \n",
    "- When an observation is incomplete, we mean that it lacks attribute values, lacks desired attributes, or is only aggregated.\n",
    "- When there are errors or outliers in the data, they are considered noisy. \n",
    "- Data that is inconsistent contains names or codes that aren't consistent with the structure of interest.\n",
    "\n",
    "## What can I do when an observations are incomplete?\n",
    "- If the dataset is large enough..., exclude all observations with missing values\n",
    "- You might consider imputed values in rare datasets (even if they are large)\n",
    "- Decide in advance (before you examine the dataset) some exclusion criteria \n",
    " \n",
    "## What can I do when there are errors or outliers in the data?\n",
    "- First task is to identify errors or outliers cases \n",
    "  - Errors are values that were logged incorrectly \n",
    "  - Outliers are values that can't be justified conceptually (i.e. using a domain expert)\n",
    "- Once you find them, you need to come up with a strategy\n",
    "  - In order to eliminate errors and outliers, we can filter data\n",
    "  - Outliers and errors can be transformed or imputed\n",
    "  - They can be investigated independently \n",
    "\n",
    "## What can I do when my dataset is inconsistent?\n",
    "It depends on the level of inconsistency? \n",
    "  - Sometimes it's all about simple hacks \n",
    "  - Sometimes you need some manual procedures \n",
    "    - In manuals, it is crucial to identify problems \n",
    "      - Understand them\n",
    "      - Handle them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is a Feature engineering?\n",
    "Feature engineering is the process of **selecting**, **manipulating**, and **transforming** raw data into features that can be used in supervised and unsupervised learning.\n",
    "\n",
    "#### What is feature manipulation? \n",
    "- Capping\n",
    "- Expansion\n",
    "- Imputation\n",
    "- Encoding\n",
    "\n",
    "\n",
    "#### What is feature transforming? \n",
    "- Scaling\n",
    "- Mathematical\n",
    "- Normal\n",
    "- Discretisation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### What is feature selection? \n",
    "The process of extracting the most consistent, non-redundant, and relevant features for use in model creation is known as feature selection. As the number and variety of datasets expand, it is critical to reduce their size methodically. The primary purpose of feature selection is to increase predictive model performance while lowering modelling computational costs.\n",
    "\n",
    "#### Feature Selection Methods\n",
    "- Feature selection algorithms are categorized as either supervised, which can be used for labeled data; or unsupervised, which can be used for unlabeled data. \n",
    "- Supervised techniques are :\n",
    "  - Directional selection\n",
    "  - Directional elimination \n",
    "  - Random selection\n",
    "  - Brute force\n",
    "- Unsupervised techniques are :\n",
    "  - Filtering\n",
    "  - Wrapping \n",
    "  - Embedding\n",
    "  - Hybrid"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af4ed3fa87ab5389ed0d164f2907ab733f2066e3ac536df37d19ad0ca2200646"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('SYS_2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
