{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility lab\n",
    "Over the last few years, there has been a surge in interest in reproducible research and data analysis pipelines. But how can I be certain that what I produce as a Python user is reproducible? In this lab, we'll take you down the rabbit hole of reproducibility. We'll go over repeatable scientific development in Python step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master notebook\n",
    "- Create a new notebook and give it the name `project_setup.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project structure\n",
    "- Good project layout guarantees \n",
    "  - Integrity of the data\n",
    "  - Portability of the project\n",
    "  - Potential for automation \n",
    "  - That the project is salvageable in the case of change in personal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic folder setup \n",
    "- There is no optimal way to organise a project\n",
    "- But by using a systematic way we simplify the process of potential automations\n",
    "- The code below gives a suggested project structure that can be adapted quite easily \n",
    "- Change it to a generalised function form that can take as optional arguments the following:\n",
    "  - project_name with `SYS_2022` being default \n",
    "  - origin with cwd being default \n",
    "  - a list of sub_folders with the current ones being the default\n",
    "- Copy the function to a python file called `utils.py` in the folder code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "origin = Path().cwd()\n",
    "project_root = f'{origin}/SYS_2022/'\n",
    "\n",
    "sub_folders = ['data/raw',\n",
    "               'config',\n",
    "               'data/processed',\n",
    "               'notebooks',\n",
    "               'code',\n",
    "               'reports',\n",
    "               'background']\n",
    "\n",
    "for folder in sub_folders:\n",
    "    tmp_folder = Path(project_root + f'/{folder}')\n",
    "    tmp_folder.mkdir(parents=True, exist_ok=True) \n",
    "    (tmp_folder / '.gitkeep').write_text(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local version control\n",
    "- We want to be absolutely certain that everything is curated by version control right from the start. \n",
    "- To do that we can use our setup notebook and the bash magic commands to issue the following git commands \n",
    "\n",
    "~~~jupyter\n",
    "%%bash\n",
    "cd SYS_2022 # goto the folder \n",
    "git ____ # Initialize the git curation \n",
    "git ____ # Add folder contents using the . notation\n",
    "git ____ # check the status of git \n",
    "# commit folder structure\n",
    "git ____ -m \"Register initial folder structure\"\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the local git log \n",
    "- The whole idea of committing changes git is that is analogous to freezing the status of your project at a certain point in time.\n",
    "- You can use the `git log` command to view the contents of the stored information just remember to `cd` inside the folder first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `.gitingore` \n",
    "- In general the `.gitignore` file tells git to not curate specific files. \n",
    "- These often include configuration files and data files. \n",
    "- The former should not be curated to avoid sharing password or private information. \n",
    "- The latter should not be curated for either violation of privacy or simply because of folder size\n",
    "- Change the following cell so it will save such a file to ignore both the contents of config and data folders  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~jupyter\n",
    "%%writefile SYS_2022/____\n",
    "____/*\n",
    "____/*\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data using the load function \n",
    "\n",
    "- Recall that in the bootcamp we downloaded the course data using the following functions: \n",
    "- Save these functions in a new module called `get_data.py` \n",
    "- Then change the `load_clean_data` function in a way that will store and load the data from the `data\\raw\\` folder location  \n",
    "\n",
    "~~~python\n",
    "def download_data():\n",
    "    tasks_url = 'https://tinyurl.com/bwfzme4r'\n",
    "    demo_url = 'https://tinyurl.com/2s4cfah6'\n",
    "    measures = pd.read_csv(tasks_url)\n",
    "    demographics = pd.read_csv(demo_url)\n",
    "    return measures,demographics\n",
    "\n",
    "def fix_measures_columns(measures):\n",
    "    col_top = [ v.split('.')[0] for v in measures.columns ]\n",
    "    col_bottom = measures.iloc[0,:].to_numpy()\n",
    "    col_top[0:2] = ['info','info']\n",
    "    col_bottom[0:2] = ('user','device')\n",
    "    multi_index = [tuple(v) for v in zip( col_top, col_bottom )]\n",
    "    measures.columns = pd.MultiIndex.from_tuples(multi_index, \n",
    "                                                 names=[\"task\", \"measure\"]) \n",
    "    measures = measures.iloc[2:,:]\n",
    "    return measures\n",
    "\n",
    "def load_clean_data():\n",
    "    import pandas as pd \n",
    "    from pathlib import Path\n",
    "    file_name = '12_tasks.pkl'\n",
    "    if Path(file_name).exists():\n",
    "        return df = pd.read_pickle(file_name)\n",
    "    else:\n",
    "        measures,demographics = download_data()\n",
    "        measures = fix_measures_columns(measures)\n",
    "        demographics = pd.concat({'info': demographics},axis=1)\n",
    "        df = pd.merge(measures.set_index(('info','user')),\n",
    "              demographics.set_index(('info','user')),\n",
    "              how='inner',\n",
    "              left_index=True,\n",
    "              right_index=True)\n",
    "        df.to_pickle(file_name)\n",
    "        return df\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for related background\n",
    "- Often in data exploration there is some material out there that gives needed background on the dataset and experimental methodology used to acquire the data.\n",
    "- It is very useful to download all relevant information in the background folder \n",
    "- It is also useful to create a markdown summary file in this folder to track these papers or tables and provide to future you or your collaborators easy access to the material\n",
    "- Use the `wget --help` command and `%%bash` magic command to figure out how to store the paper sharing the data in the background folder ([here is the link](https://www.researchsquare.com/article/rs-373663/v1.pdf)) \n",
    "- Another important issue is looking at the expected distributions to interpret the results \n",
    "  - For example [cbs health interpretation guide](https://www.cambridgebrainsciences.com/assets/resources/cbs-health-interpretation-guide-1544198242.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start filling up a README.md \n",
    "In any repository a Readme file is the window to the project, any high level information should be included there. \n",
    "- Using either the `writefile` magic command or the Path object create a `Readme.md` containing the following:\n",
    "  - A project name \n",
    "  - Team members (while you can do this project alone it will be easier to do as a group)\n",
    "  - Dataset info:\n",
    "    - Dataset title  \n",
    "    - Links to supplementary material\n",
    "    - Related publications \n",
    "    - Where was the data collected at?\n",
    "    - What is the experimental methodology? \n",
    "    - When was the data collected? \n",
    "    - What time span does the data cover?\n",
    "    - How was the data collected?\n",
    "    - How was the data processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is also useful to create a dataset key\n",
    "- In principle, a dataset key ought to be included in each and every shared data collection. \n",
    "- This is simply a file that contains the metadata for each columns header.  \n",
    "- In psychology columns are often referred to as factors \n",
    "- In machine learning, the same thing is called a feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metadata is a complex issue \n",
    "- We tend to ignore the time and effort it takes in researching the different crucial information about each feature\n",
    "- In large datasets containing thousands of features maintaining a metadata store is important \n",
    "- [Here is a link that explains metadata complexity](https://specs.frictionlessdata.io//data-package/#metadata)\n",
    "- Like with everything in Python, a specialised module was developed particularly for that purpose (see [frictionless](https://framework.frictionlessdata.io/))\n",
    "- However, for small datasets creating a key file is a simple task \n",
    "- Either use the magic `%%writefile` command to create a a `.yaml` file to store important information about your features, or just create one and store the bits of information you think is key. \n",
    "\n",
    "#### What are .yml/.yaml files \n",
    "\n",
    "Configuration files are often written in YAML, a language for serialising data. You can find YAML as yet another markup language or as [(YAML)](https://en.wikipedia.org/wiki/YAML) YAML Ain't Markup Language (a recursive acronym), emphasising that YAML is for data, not documents. Simply put, `.yaml` files are a simple format for bridging the gap between humans and computers by creating data files that are easy to read and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What metadata should you store?\n",
    "- It depends on the context\n",
    "- In our case I would store for a task the following:\n",
    "  - Description of the task \n",
    "  - Common metrics per task and their interpretation \n",
    "  - Unit per metric\n",
    "- For demographics I will try to record the question used to collect that feature\n",
    "- In the context of this dataset you have 12 tasks and a demographic column if you split those among you you can create a very detailed key in no time. \n",
    "\n",
    "### Making sense of cognitive tasks \n",
    "- Cognitive tasks are confusing \n",
    "- There are multiple names that are testing almost exactly the same construct using either different wording or very subtle differences. \n",
    "- When you are trying to make sense of an external dataset it is often useful to try to link the exotic task to one that is more established in the literature \n",
    "- The [Cognitive Atlas](http://www.cognitiveatlas.org/) \n",
    "\n",
    "- The complexity of metadata also exists in Neurophysiology. E.g. [Handling Metadata in a Neurophysiology Laboratory](https://www.frontiersin.org/articles/10.3389/fninf.2016.00026/full)\n",
    "\n",
    "- Or in Neuroscience see [openMINDS](https://github.com/HumanBrainProject/openMINDS)\n",
    "\n",
    "- In fact any field in the behavioural domains will require some diving into \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis  \n",
    "- Exploratory research (sometimes called hypothesis generating) is about finding how variables influence each other. This approach means the researcher doesn't have any preconceived ideas or assumptions.  \n",
    "- A confirmatory study (also known as hypothesis testing) is one in which the researcher has a pretty clear idea about the relationship between variables. Here, the researcher is trying to find out if a theory, or hypotheses, is supported by data.  \n",
    "- The same data can be viewed from different angles, but as datasets become more complex (i.e. more factors are present), it is very challenging to conduct pure exploratory research and some expectations should exist about strong relationships previously described in the literature. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual task \n",
    "- Select five demographic features of interest and at least three different cognitive tasks to create a dataset subset that will be used to conduct exploratory data analysis. \n",
    "- For example:  \n",
    "  - The following code block examines the within and between global association across tasks and their metrics \n",
    "\n",
    "~~~python\n",
    "import seaborn as sns \n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "df = pd.read_pickle('SYS_2022/data/raw/12_tasks.pkl')\n",
    "tasks = df.xs(['spatial_planning','spatial_span','feature_match'],1).astype(float)\n",
    "sns.heatmap(tasks.corr());\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another common question relates to the level of completeness across the different metrics you acquired \n",
    "- If our objective is to increase the number of observation to the maximum it might be useful to identify the landscape of completeness across the tasks \n",
    "- Using the same heatmap plot together with pandas try to create the following plot that visualizes the per task completeness \n",
    "\n",
    "![](completeness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once you have selected those create a selected set of summary plots that reflect any \"interesting\" questions that relate to potential association that you would like to investigate further "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add markdown section that bring your interpretation to the plot "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}